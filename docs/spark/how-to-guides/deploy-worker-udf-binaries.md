---
title: 針對 Apache Spark 的背景工作角色和使用者定義的函數二進位檔部署 .NET
description: 瞭解如何部署適用于 Apache Spark 背景工作角色和使用者定義函數二進位檔的 .NET。
ms.date: 01/21/2019
ms.topic: conceptual
ms.custom: mvc,how-to
ms.openlocfilehash: f9197ca3cf8066f0849ebbe70d7757c9035d02f6
ms.sourcegitcommit: de17a7a0a37042f0d4406f5ae5393531caeb25ba
ms.translationtype: MT
ms.contentlocale: zh-TW
ms.lasthandoff: 01/24/2020
ms.locfileid: "76748528"
---
# <a name="deploy-net-for-apache-spark-worker-and-user-defined-function-binaries"></a><span data-ttu-id="160ec-103">針對 Apache Spark 的背景工作角色和使用者定義的函數二進位檔部署 .NET</span><span class="sxs-lookup"><span data-stu-id="160ec-103">Deploy .NET for Apache Spark worker and user-defined function binaries</span></span>

<span data-ttu-id="160ec-104">本 how to 提供如何針對 Apache Spark worker 和使用者定義函數二進位檔部署 .NET 的一般指示。</span><span class="sxs-lookup"><span data-stu-id="160ec-104">This how-to provides general instructions on how to deploy .NET for Apache Spark worker and user-defined function binaries.</span></span> <span data-ttu-id="160ec-105">您會瞭解要設定的環境變數，以及用來啟動具有 `spark-submit`之應用程式的一些常用參數。</span><span class="sxs-lookup"><span data-stu-id="160ec-105">You learn which Environment Variables to set up, as well as some commonly used parameters for launching applications with `spark-submit`.</span></span>

## <a name="configurations"></a><span data-ttu-id="160ec-106">設定</span><span class="sxs-lookup"><span data-stu-id="160ec-106">Configurations</span></span>
<span data-ttu-id="160ec-107">設定會顯示一般環境變數和參數設定，以便為 Apache Spark 的背景工作角色和使用者定義的函數二進位檔部署 .NET。</span><span class="sxs-lookup"><span data-stu-id="160ec-107">Configurations show the general environment variables and parameters settings in order to deploy .NET for Apache Spark worker and user-defined function binaries.</span></span>

### <a name="environment-variables"></a><span data-ttu-id="160ec-108">環境變數</span><span class="sxs-lookup"><span data-stu-id="160ec-108">Environment variables</span></span>
<span data-ttu-id="160ec-109">部署背景工作角色和寫入 Udf 時，您可能需要設定幾個常用的環境變數：</span><span class="sxs-lookup"><span data-stu-id="160ec-109">When deploying workers and writing UDFs, there are a few commonly used environment variables that you may need to set:</span></span> 

| <span data-ttu-id="160ec-110">環境變數</span><span class="sxs-lookup"><span data-stu-id="160ec-110">Environment Variable</span></span>         | <span data-ttu-id="160ec-111">描述</span><span class="sxs-lookup"><span data-stu-id="160ec-111">Description</span></span>
| :--------------------------- | :---------- 
| <span data-ttu-id="160ec-112">DOTNET_WORKER_DIR</span><span class="sxs-lookup"><span data-stu-id="160ec-112">DOTNET_WORKER_DIR</span></span>            | <span data-ttu-id="160ec-113">已產生 <code>Microsoft.Spark.Worker</code> 二進位檔的路徑。</span><span class="sxs-lookup"><span data-stu-id="160ec-113">Path where the <code>Microsoft.Spark.Worker</code> binary has been generated.</span></span></br><span data-ttu-id="160ec-114">它是由 Spark 驅動程式使用，並會傳遞給 Spark 執行程式。</span><span class="sxs-lookup"><span data-stu-id="160ec-114">It's used by the Spark driver and will be passed to Spark executors.</span></span> <span data-ttu-id="160ec-115">如果未設定此變數，Spark 執行執行會搜尋 <code>PATH</code> 環境變數中指定的路徑。</span><span class="sxs-lookup"><span data-stu-id="160ec-115">If this variable is not set up, the Spark executors will search the path specified in the <code>PATH</code> environment variable.</span></span></br><span data-ttu-id="160ec-116">_例如 "C:\bin\Microsoft.Spark.Worker"_</span><span class="sxs-lookup"><span data-stu-id="160ec-116">_e.g. "C:\bin\Microsoft.Spark.Worker"_</span></span>
| <span data-ttu-id="160ec-117">DOTNET_ASSEMBLY_SEARCH_PATHS</span><span class="sxs-lookup"><span data-stu-id="160ec-117">DOTNET_ASSEMBLY_SEARCH_PATHS</span></span> | <span data-ttu-id="160ec-118">以逗號分隔的路徑，其中 <code>Microsoft.Spark.Worker</code> 會載入元件。</span><span class="sxs-lookup"><span data-stu-id="160ec-118">Comma-separated paths where <code>Microsoft.Spark.Worker</code> will load assemblies.</span></span></br><span data-ttu-id="160ec-119">請注意，如果路徑是以 "." 開頭，則會在前面加上工作目錄。</span><span class="sxs-lookup"><span data-stu-id="160ec-119">Note that if a path starts with ".", the working directory will be prepended.</span></span> <span data-ttu-id="160ec-120">如果在**yarn 模式**中，"." 會代表容器的工作目錄。</span><span class="sxs-lookup"><span data-stu-id="160ec-120">If in **yarn mode**, "." would represent the container's working directory.</span></span></br><span data-ttu-id="160ec-121">_例如，"C:\Users\\&lt;使用者名稱&gt;\\&lt;mysparkapp&gt;\bin\Debug\\&lt;dotnet 版本&gt;"_</span><span class="sxs-lookup"><span data-stu-id="160ec-121">_e.g. "C:\Users\\&lt;user name&gt;\\&lt;mysparkapp&gt;\bin\Debug\\&lt;dotnet version&gt;"_</span></span>
| <span data-ttu-id="160ec-122">DOTNET_WORKER_DEBUG</span><span class="sxs-lookup"><span data-stu-id="160ec-122">DOTNET_WORKER_DEBUG</span></span>          | <span data-ttu-id="160ec-123">如果您想要對<a href="https://github.com/dotnet/spark/blob/master/docs/developer-guide.md#debugging-user-defined-function-udf">UDF 進行調試</a>，請在執行 <code>spark-submit</code>之前，將此環境變數設定為 <code>1</code>。</span><span class="sxs-lookup"><span data-stu-id="160ec-123">If you want to <a href="https://github.com/dotnet/spark/blob/master/docs/developer-guide.md#debugging-user-defined-function-udf">debug a UDF</a>, then set this environment variable to <code>1</code> before running <code>spark-submit</code>.</span></span>

### <a name="parameter-options"></a><span data-ttu-id="160ec-124">參數選項</span><span class="sxs-lookup"><span data-stu-id="160ec-124">Parameter options</span></span>
<span data-ttu-id="160ec-125">一旦將 Spark 應用程式[配套](https://spark.apache.org/docs/latest/submitting-applications.html#bundling-your-applications-dependencies)之後，您就可以使用 `spark-submit`來啟動它。</span><span class="sxs-lookup"><span data-stu-id="160ec-125">Once the Spark application is [bundled](https://spark.apache.org/docs/latest/submitting-applications.html#bundling-your-applications-dependencies), you can launch it using `spark-submit`.</span></span> <span data-ttu-id="160ec-126">下表顯示一些常用的選項：</span><span class="sxs-lookup"><span data-stu-id="160ec-126">The following table shows some of the commonly used options:</span></span> 

| <span data-ttu-id="160ec-127">參數名稱</span><span class="sxs-lookup"><span data-stu-id="160ec-127">Parameter Name</span></span>        | <span data-ttu-id="160ec-128">描述</span><span class="sxs-lookup"><span data-stu-id="160ec-128">Description</span></span>
| :---------------------| :---------- 
| <span data-ttu-id="160ec-129">--class</span><span class="sxs-lookup"><span data-stu-id="160ec-129">--class</span></span>               | <span data-ttu-id="160ec-130">應用程式的進入點。</span><span class="sxs-lookup"><span data-stu-id="160ec-130">The entry point for your application.</span></span></br><span data-ttu-id="160ec-131">_例如，dotnet. DotnetRunner。_</span><span class="sxs-lookup"><span data-stu-id="160ec-131">_e.g. org.apache.spark.deploy.dotnet.DotnetRunner_</span></span>
| <span data-ttu-id="160ec-132">--master</span><span class="sxs-lookup"><span data-stu-id="160ec-132">--master</span></span>              | <span data-ttu-id="160ec-133">叢集的<a href="https://spark.apache.org/docs/latest/submitting-applications.html#master-urls">主要 URL</a> 。</span><span class="sxs-lookup"><span data-stu-id="160ec-133">The <a href="https://spark.apache.org/docs/latest/submitting-applications.html#master-urls">master URL</a> for the cluster.</span></span></br><span data-ttu-id="160ec-134">_例如 yarn_</span><span class="sxs-lookup"><span data-stu-id="160ec-134">_e.g. yarn_</span></span>
| <span data-ttu-id="160ec-135">--部署模式</span><span class="sxs-lookup"><span data-stu-id="160ec-135">--deploy-mode</span></span>         | <span data-ttu-id="160ec-136">是否要將您的驅動程式部署在背景工作角色節點（<code>cluster</code>）或本機上做為外部用戶端（<code>client</code>）。</span><span class="sxs-lookup"><span data-stu-id="160ec-136">Whether to deploy your driver on the worker nodes (<code>cluster</code>) or locally as an external client (<code>client</code>).</span></span></br><span data-ttu-id="160ec-137">預設：<code>client</code></span><span class="sxs-lookup"><span data-stu-id="160ec-137">Default: <code>client</code></span></span>
| <span data-ttu-id="160ec-138">--會議</span><span class="sxs-lookup"><span data-stu-id="160ec-138">--conf</span></span>                | <span data-ttu-id="160ec-139"><code>key=value</code> 格式的任意 Spark 設定屬性。</span><span class="sxs-lookup"><span data-stu-id="160ec-139">Arbitrary Spark configuration property in <code>key=value</code> format.</span></span></br><span data-ttu-id="160ec-140">_例如，yarn. appMasterEnv. DOTNET_WORKER_DIR = .\worker\Microsoft.Spark.Worker_</span><span class="sxs-lookup"><span data-stu-id="160ec-140">_e.g. spark.yarn.appMasterEnv.DOTNET_WORKER_DIR=.\worker\Microsoft.Spark.Worker_</span></span>
| <span data-ttu-id="160ec-141">--files</span><span class="sxs-lookup"><span data-stu-id="160ec-141">--files</span></span>               | <span data-ttu-id="160ec-142">要放入每個執行程式之工作目錄中的檔案清單（以逗號分隔）。</span><span class="sxs-lookup"><span data-stu-id="160ec-142">Comma-separated list of files to be placed in the working directory of each executor.</span></span><br/><ul><li><span data-ttu-id="160ec-143">請注意，此選項僅適用于 yarn 模式。</span><span class="sxs-lookup"><span data-stu-id="160ec-143">Please note that this option is only applicable for yarn mode.</span></span></li><li><span data-ttu-id="160ec-144">它支援使用與 Hadoop 相似的 # 來指定檔案名。</span><span class="sxs-lookup"><span data-stu-id="160ec-144">It supports specifying file names with # similar to Hadoop.</span></span></br></ul><span data-ttu-id="160ec-145">_例如 <code>myLocalSparkApp.dll#appSeen.dll</code>。在 YARN 上執行時，您的應用程式應該使用名稱做為 <code>appSeen.dll</code> 來參考 <code>myLocalSparkApp.dll</code>。_</span><span class="sxs-lookup"><span data-stu-id="160ec-145">_e.g. <code>myLocalSparkApp.dll#appSeen.dll</code>. Your application should use the name as <code>appSeen.dll</code> to reference <code>myLocalSparkApp.dll</code> when running on YARN._</span></span></li>
| <span data-ttu-id="160ec-146">--封存</span><span class="sxs-lookup"><span data-stu-id="160ec-146">--archives</span></span>          | <span data-ttu-id="160ec-147">要解壓縮到每個執行程式工作目錄的封存清單（以逗號分隔）。</span><span class="sxs-lookup"><span data-stu-id="160ec-147">Comma-separated list of archives to be extracted into the working directory of each executor.</span></span></br><ul><li><span data-ttu-id="160ec-148">請注意，此選項僅適用于 yarn 模式。</span><span class="sxs-lookup"><span data-stu-id="160ec-148">Please note that this option is only applicable for yarn mode.</span></span></li><li><span data-ttu-id="160ec-149">它支援使用與 Hadoop 相似的 # 來指定檔案名。</span><span class="sxs-lookup"><span data-stu-id="160ec-149">It supports specifying file names with # similar to Hadoop.</span></span></br></ul><span data-ttu-id="160ec-150">_例如 <code>hdfs://&lt;path to your worker file&gt;/Microsoft.Spark.Worker.zip#worker</code>。這會將 zip 檔案複製並解壓縮至 <code>worker</code> 資料夾。_</span><span class="sxs-lookup"><span data-stu-id="160ec-150">_e.g. <code>hdfs://&lt;path to your worker file&gt;/Microsoft.Spark.Worker.zip#worker</code>. This will copy and extract the zip file to <code>worker</code> folder._</span></span></li>
| <span data-ttu-id="160ec-151">應用程式-jar</span><span class="sxs-lookup"><span data-stu-id="160ec-151">application-jar</span></span>       | <span data-ttu-id="160ec-152">配套的 jar 路徑，包括您的應用程式和所有相依性。</span><span class="sxs-lookup"><span data-stu-id="160ec-152">Path to a bundled jar including your application and all dependencies.</span></span></br><span data-ttu-id="160ec-153">_例如，hdfs://&lt;您的 jar&gt;/microsoft-spark-&lt;版本&gt;.jar 的路徑_</span><span class="sxs-lookup"><span data-stu-id="160ec-153">_e.g. hdfs://&lt;path to your jar&gt;/microsoft-spark-&lt;version&gt;.jar_</span></span>
| <span data-ttu-id="160ec-154">應用程式引數</span><span class="sxs-lookup"><span data-stu-id="160ec-154">application-arguments</span></span> | <span data-ttu-id="160ec-155">傳遞至主要類別之 main 方法的引數（如果有的話）。</span><span class="sxs-lookup"><span data-stu-id="160ec-155">Arguments passed to the main method of your main class, if any.</span></span></br><span data-ttu-id="160ec-156">_例如，hdfs://應用程式&lt;路徑&gt;/&lt;您的應用程式&gt;應用程式名稱 &lt;&gt; 應用程式引數 &lt;_ &gt;</span><span class="sxs-lookup"><span data-stu-id="160ec-156">_e.g. hdfs://&lt;path to your app&gt;/&lt;your app&gt;.zip &lt;your app name&gt; &lt;app args&gt;_</span></span>

> [!NOTE]
> <span data-ttu-id="160ec-157">使用 `spark-submit`啟動應用程式之前，請 `application-jar` 先指定所有的 `--options`，否則會予以忽略。</span><span class="sxs-lookup"><span data-stu-id="160ec-157">Specify all the `--options` before `application-jar` when launching applications with `spark-submit`, otherwise they will be ignored.</span></span> <span data-ttu-id="160ec-158">如需詳細資訊，請參閱[`spark-submit` 選項](https://spark.apache.org/docs/latest/submitting-applications.html)和[在 YARN 上執行 spark 詳細資料](https://spark.apache.org/docs/latest/running-on-yarn.html)。</span><span class="sxs-lookup"><span data-stu-id="160ec-158">For more information, see [`spark-submit` options](https://spark.apache.org/docs/latest/submitting-applications.html) and [running spark on YARN details](https://spark.apache.org/docs/latest/running-on-yarn.html).</span></span>

## <a name="frequently-asked-questions"></a><span data-ttu-id="160ec-159">常見問題集</span><span class="sxs-lookup"><span data-stu-id="160ec-159">Frequently asked questions</span></span>
### <a name="when-i-run-a-spark-app-with-udfs-i-get-a-filenotfoundexception-error-what-should-i-do"></a><span data-ttu-id="160ec-160">當我使用 Udf 執行 spark 應用程式時，會收到「FileNotFoundException」錯誤。</span><span class="sxs-lookup"><span data-stu-id="160ec-160">When I run a spark app with UDFs, I get a \`FileNotFoundException' error.</span></span> <span data-ttu-id="160ec-161">我該怎麼做？</span><span class="sxs-lookup"><span data-stu-id="160ec-161">What should I do?</span></span>
> <span data-ttu-id="160ec-162">**錯誤：** [錯誤] [TaskRunner] [0] ProcessStream （）失敗，發生例外狀況： FileNotFoundException：元件 ' MySparkApp，版本 = 1.0.0.0，文化特性 = 中性，PublicKeyToken = null ' 找不到檔案： ' mySparkApp .dll '</span><span class="sxs-lookup"><span data-stu-id="160ec-162">**Error:** [Error] [TaskRunner] [0] ProcessStream() failed with exception: System.IO.FileNotFoundException: Assembly 'mySparkApp, Version=1.0.0.0, Culture=neutral, PublicKeyToken=null' file not found: 'mySparkApp.dll'</span></span>

<span data-ttu-id="160ec-163">**答：** 檢查是否已正確設定 `DOTNET_ASSEMBLY_SEARCH_PATHS` 環境變數。</span><span class="sxs-lookup"><span data-stu-id="160ec-163">**Answer:** Check that the `DOTNET_ASSEMBLY_SEARCH_PATHS` environment variable is set correctly.</span></span> <span data-ttu-id="160ec-164">它應該是包含您 `mySparkApp.dll`的路徑。</span><span class="sxs-lookup"><span data-stu-id="160ec-164">It should be the path that contains your `mySparkApp.dll`.</span></span>

### <a name="after-i-upgraded-my-net-for-apache-spark-version-and-reset-the-dotnet_worker_dir-environment-variable-why-do-i-still-get-the-following-ioexception-error"></a><span data-ttu-id="160ec-165">升級 Apache Spark 版本的 .NET 並重設 `DOTNET_WORKER_DIR` 環境變數之後，為什麼仍然會出現下列 `IOException` 錯誤？</span><span class="sxs-lookup"><span data-stu-id="160ec-165">After I upgraded my .NET for Apache Spark version and reset the `DOTNET_WORKER_DIR` environment variable, why do I still get the following `IOException` error?</span></span>
> <span data-ttu-id="160ec-166">**錯誤：** 在階段11.0 中遺失工作0.0 （TID 24、localhost、執行器驅動程式）： IOException：無法執行程式 "nuget.exe"： CreateProcess 錯誤 = 2，系統找不到指定的檔案。</span><span class="sxs-lookup"><span data-stu-id="160ec-166">**Error:** Lost task 0.0 in stage 11.0 (TID 24, localhost, executor driver): java.io.IOException: Cannot run program "Microsoft.Spark.Worker.exe": CreateProcess error=2, The system cannot find the file specified.</span></span>

<span data-ttu-id="160ec-167">**答：** 請先嘗試重新開機您的 PowerShell 視窗（或其他命令視窗），使其可以接受最新的環境變數值。</span><span class="sxs-lookup"><span data-stu-id="160ec-167">**Answer:** Try restarting your PowerShell window (or other command windows) first so that it can take the latest environment variable values.</span></span> <span data-ttu-id="160ec-168">然後啟動您的程式。</span><span class="sxs-lookup"><span data-stu-id="160ec-168">Then start your program.</span></span>

### <a name="after-submitting-my-spark-application-i-get-the-error-systemtypeloadexception-could-not-load-type-systemruntimeremotingcontextscontext"></a><span data-ttu-id="160ec-169">提交 Spark 應用程式之後，我收到 `System.TypeLoadException: Could not load type 'System.Runtime.Remoting.Contexts.Context'`的錯誤。</span><span class="sxs-lookup"><span data-stu-id="160ec-169">After submitting my Spark application, I get the error `System.TypeLoadException: Could not load type 'System.Runtime.Remoting.Contexts.Context'`.</span></span>
> <span data-ttu-id="160ec-170">**錯誤：** [錯誤] [TaskRunner] [0] ProcessStream （）失敗，發生例外狀況： TypeLoadException：無法從元件 ' Mscorlib，Version = 4.0.0.0，Culture = 中性，PublicKeyToken = ... ' 載入類型 ' system.object '。</span><span class="sxs-lookup"><span data-stu-id="160ec-170">**Error:** [Error] [TaskRunner] [0] ProcessStream() failed with exception: System.TypeLoadException: Could not load type 'System.Runtime.Remoting.Contexts.Context' from assembly 'mscorlib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=...'.</span></span>

<span data-ttu-id="160ec-171">**答：** 檢查您所使用的 `Microsoft.Spark.Worker` 版本。</span><span class="sxs-lookup"><span data-stu-id="160ec-171">**Answer:** Check the `Microsoft.Spark.Worker` version you are using.</span></span> <span data-ttu-id="160ec-172">有兩種版本： **.NET Framework 4.6.1**和 **.net Core 2.1. x**。</span><span class="sxs-lookup"><span data-stu-id="160ec-172">There are two versions: **.NET Framework 4.6.1** and **.NET Core 2.1.x**.</span></span> <span data-ttu-id="160ec-173">在此情況下，應該使用 `Microsoft.Spark.Worker.net461.win-x64-<version>` （您可以[下載](https://github.com/dotnet/spark/releases)），因為 `System.Runtime.Remoting.Contexts.Context` 僅適用于 .NET Framework。</span><span class="sxs-lookup"><span data-stu-id="160ec-173">In this case, `Microsoft.Spark.Worker.net461.win-x64-<version>` (which you can [download](https://github.com/dotnet/spark/releases)) should be used since `System.Runtime.Remoting.Contexts.Context` is only for .NET Framework.</span></span>

### <a name="how-do-i-run-my-spark-application-with-udfs-on-yarn-which-environment-variables-and-parameters-should-i-use"></a><span data-ttu-id="160ec-174">如何? 在 YARN 上使用 Udf 執行我的 spark 應用程式？</span><span class="sxs-lookup"><span data-stu-id="160ec-174">How do I run my spark application with UDFs on YARN?</span></span> <span data-ttu-id="160ec-175">我應該使用哪些環境變數與參數？</span><span class="sxs-lookup"><span data-stu-id="160ec-175">Which environment variables and parameters should I use?</span></span>

<span data-ttu-id="160ec-176">**答：** 若要在 YARN 上啟動 spark 應用程式，應將環境變數指定為 `spark.yarn.appMasterEnv.[EnvironmentVariableName]`。</span><span class="sxs-lookup"><span data-stu-id="160ec-176">**Answer:** To launch the spark application on YARN, the environment variables should be specified as `spark.yarn.appMasterEnv.[EnvironmentVariableName]`.</span></span> <span data-ttu-id="160ec-177">請參閱下面的範例，以使用 `spark-submit`：</span><span class="sxs-lookup"><span data-stu-id="160ec-177">Please see below as an example using `spark-submit`:</span></span>

```powershell
spark-submit \
--class org.apache.spark.deploy.dotnet.DotnetRunner \
--master yarn \
--deploy-mode cluster \
--conf spark.yarn.appMasterEnv.DOTNET_WORKER_DIR=./worker/Microsoft.Spark.Worker-<version> \
--conf spark.yarn.appMasterEnv.DOTNET_ASSEMBLY_SEARCH_PATHS=./udfs \
--archives hdfs://<path to your files>/Microsoft.Spark.Worker.net461.win-x64-<version>.zip#worker,hdfs://<path to your files>/mySparkApp.zip#udfs \
hdfs://<path to jar file>/microsoft-spark-2.4.x-<version>.jar \
hdfs://<path to your files>/mySparkApp.zip mySparkApp
```

## <a name="next-steps"></a><span data-ttu-id="160ec-178">後續步驟</span><span class="sxs-lookup"><span data-stu-id="160ec-178">Next steps</span></span>

* [<span data-ttu-id="160ec-179">開始使用適用於 Apache Spark 的 .NET</span><span class="sxs-lookup"><span data-stu-id="160ec-179">Get started with .NET for Apache Spark</span></span>](../tutorials/get-started.md)
* [<span data-ttu-id="160ec-180">在 Windows 上針對 Apache Spark 應用程式的 .NET 進行 Debug</span><span class="sxs-lookup"><span data-stu-id="160ec-180">Debug a .NET for Apache Spark application on Windows</span></span>](../how-to-guides/debug.md)
